"""
Advanced Analysis Service - Ph√¢n t√≠ch chuy√™n s√¢u fake news.
Cung c·∫•p c√°c ph√¢n t√≠ch n√¢ng cao v·ªÅ:
- Source credibility (ƒë·ªô tin c·∫≠y ngu·ªìn tin)
- Trend analysis (xu h∆∞·ªõng fake news)
- Content analysis (ph√¢n t√≠ch n·ªôi dung)
- Risk assessment (ƒë√°nh gi√° r·ªßi ro)
"""
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from collections import Counter
import re
import statistics

from app.core.database import mongodb
from app.core.logger import get_logger

logger = get_logger(__name__)


class AdvancedAnalysisService:
    """Service ph√¢n t√≠ch chuy√™n s√¢u fake news."""
    
    # ========================
    # SOURCE CREDIBILITY
    # ========================
    
    @staticmethod
    async def get_source_credibility_score(
        domain: str,
        min_posts: int = 5
    ) -> Dict[str, Any]:
        """
        T√≠nh ƒëi·ªÉm ƒë·ªô tin c·∫≠y c·ªßa m·ªôt ngu·ªìn tin (domain).
        
        Scoring factors:
        - Fake ratio (t·ª∑ l·ªá fake news)
        - Average confidence (ƒë·ªô ch·∫Øc ch·∫Øn c·ªßa model)
        - Engagement ratio (t∆∞∆°ng t√°c th·∫≠t vs fake)
        - Post volume (s·ªë l∆∞·ª£ng posts)
        
        Returns:
            Dict v·ªõi credibility_score (0-100) v√† breakdown
        """
        try:
            collection = mongodb.get_collection("reddit_posts")
            
            # Get all posts from this domain with predictions
            posts = await collection.find({
                "domain": domain,
                "prediction": {"$exists": True, "$ne": None}
            }).to_list(length=10000)
            
            if len(posts) < min_posts:
                return {
                    "domain": domain,
                    "credibility_score": None,
                    "message": f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu (c·∫ßn √≠t nh·∫•t {min_posts} posts)",
                    "total_posts": len(posts)
                }
            
            # Calculate metrics
            fake_posts = [p for p in posts if p.get("prediction", {}).get("label") == "FAKE"]
            real_posts = [p for p in posts if p.get("prediction", {}).get("label") == "REAL"]
            
            fake_ratio = len(fake_posts) / len(posts) if posts else 0
            
            # Average confidence for fake news
            fake_confidences = [p.get("prediction", {}).get("confidence", 0) for p in fake_posts]
            avg_fake_confidence = statistics.mean(fake_confidences) if fake_confidences else 0
            
            # Engagement metrics
            fake_avg_score = statistics.mean([p.get("score", 0) for p in fake_posts]) if fake_posts else 0
            real_avg_score = statistics.mean([p.get("score", 0) for p in real_posts]) if real_posts else 0
            
            # Calculate credibility score (0-100, higher is more credible)
            # Formula: Base 100, subtract penalties for fake news
            credibility_score = 100
            
            # Penalty for fake ratio (max -50 points)
            credibility_score -= fake_ratio * 50
            
            # Penalty for high confidence fake news (max -20 points)
            credibility_score -= avg_fake_confidence * 20
            
            # Bonus for having mostly real news with high engagement
            if real_avg_score > fake_avg_score and len(real_posts) > len(fake_posts):
                credibility_score += 10
            
            # Clamp to 0-100
            credibility_score = max(0, min(100, credibility_score))
            
            # Risk level
            if credibility_score >= 80:
                risk_level = "LOW"
                risk_color = "green"
            elif credibility_score >= 60:
                risk_level = "MEDIUM"
                risk_color = "yellow"
            elif credibility_score >= 40:
                risk_level = "HIGH"
                risk_color = "orange"
            else:
                risk_level = "VERY_HIGH"
                risk_color = "red"
            
            return {
                "domain": domain,
                "credibility_score": round(credibility_score, 2),
                "risk_level": risk_level,
                "risk_color": risk_color,
                "breakdown": {
                    "total_posts": len(posts),
                    "fake_posts": len(fake_posts),
                    "real_posts": len(real_posts),
                    "fake_ratio": round(fake_ratio, 4),  # Ratio 0-1
                    "fake_percentage": round(fake_ratio * 100, 2),  # Percentage 0-100
                    "avg_fake_confidence": round(avg_fake_confidence * 100, 2),
                    "fake_avg_score": round(fake_avg_score, 2),
                    "real_avg_score": round(real_avg_score, 2)
                },
                "recommendation": AdvancedAnalysisService._get_source_recommendation(credibility_score)
            }
            
        except Exception as e:
            logger.error(f"Error calculating source credibility: {e}")
            return {"error": str(e), "domain": domain}
    
    @staticmethod
    def _get_source_recommendation(score: float) -> str:
        """Generate recommendation based on credibility score."""
        if score >= 80:
            return "Ngu·ªìn tin ƒë√°ng tin c·∫≠y. C√≥ th·ªÉ tham kh·∫£o nh∆∞ng v·∫´n n√™n ƒë·ªëi chi·∫øu."
        elif score >= 60:
            return "Ngu·ªìn tin c√≥ ƒë·ªô tin c·∫≠y trung b√¨nh. N√™n ƒë·ªëi chi·∫øu v·ªõi nhi·ªÅu ngu·ªìn kh√°c."
        elif score >= 40:
            return "Ngu·ªìn tin c√≥ r·ªßi ro cao. C·∫ßn ki·ªÉm ch·ª©ng k·ªπ tr∆∞·ªõc khi tin."
        else:
            return "Ngu·ªìn tin kh√¥ng ƒë√°ng tin c·∫≠y. Khuy·∫øn ngh·ªã kh√¥ng s·ª≠ d·ª•ng."
    
    @staticmethod
    async def get_top_credible_sources(
        limit: int = 20,
        min_posts: int = 10
    ) -> List[Dict[str, Any]]:
        """
        L·∫•y danh s√°ch c√°c ngu·ªìn tin ƒë√°ng tin c·∫≠y nh·∫•t.
        """
        try:
            collection = mongodb.get_collection("reddit_posts")
            
            # Aggregate by domain
            pipeline = [
                {"$match": {"prediction": {"$exists": True, "$ne": None}}},
                {
                    "$group": {
                        "_id": "$domain",
                        "total": {"$sum": 1},
                        "fake_count": {
                            "$sum": {"$cond": [{"$eq": ["$prediction.label", "FAKE"]}, 1, 0]}
                        },
                        "real_count": {
                            "$sum": {"$cond": [{"$eq": ["$prediction.label", "REAL"]}, 1, 0]}
                        },
                        "avg_confidence": {"$avg": "$prediction.confidence"}
                    }
                },
                {"$match": {"total": {"$gte": min_posts}}},
                {"$addFields": {
                    "fake_ratio": {"$divide": ["$fake_count", "$total"]},
                    "credibility": {
                        "$subtract": [
                            100,
                            {"$multiply": [{"$divide": ["$fake_count", "$total"]}, 50]}
                        ]
                    }
                }},
                {"$sort": {"credibility": -1}},
                {"$limit": limit}
            ]
            
            # PyMongo 4.10+ aggregate() is a coroutine, need to await it first
            cursor = await collection.aggregate(pipeline)
            results = await cursor.to_list(length=limit)
            
            sources = []
            for item in results:
                credibility = item.get("credibility", 0)
                if credibility >= 80:
                    risk_level = "LOW"
                elif credibility >= 60:
                    risk_level = "MEDIUM"
                elif credibility >= 40:
                    risk_level = "HIGH"
                else:
                    risk_level = "VERY_HIGH"
                
                sources.append({
                    "domain": item["_id"],
                    "credibility_score": round(credibility, 2),
                    "risk_level": risk_level,
                    "risk_color": "green" if risk_level == "LOW" else "yellow" if risk_level == "MEDIUM" else "orange" if risk_level == "HIGH" else "red",
                    "breakdown": {
                        "total_posts": item["total"],
                        "fake_posts": item["fake_count"],
                        "real_posts": item["real_count"],
                        "fake_ratio": round(item["fake_ratio"], 4),  # Ratio 0-1
                        "fake_percentage": round(item["fake_ratio"] * 100, 2),  # Percentage 0-100
                        "avg_fake_confidence": 0,  # Not calculated in this endpoint
                        "fake_avg_score": 0,  # Not calculated in this endpoint
                        "real_avg_score": 0  # Not calculated in this endpoint
                    },
                    "recommendation": AdvancedAnalysisService._get_source_recommendation(credibility)
                })
            
            return sources
            
        except Exception as e:
            logger.error(f"Error getting top credible sources: {e}")
            return []
    
    @staticmethod
    async def get_least_credible_sources(
        limit: int = 20,
        min_posts: int = 5
    ) -> List[Dict[str, Any]]:
        """
        L·∫•y danh s√°ch c√°c ngu·ªìn tin √≠t ƒë√°ng tin c·∫≠y nh·∫•t (nhi·ªÅu fake news nh·∫•t).
        """
        try:
            collection = mongodb.get_collection("reddit_posts")
            
            pipeline = [
                {"$match": {"prediction": {"$exists": True, "$ne": None}}},
                {
                    "$group": {
                        "_id": "$domain",
                        "total": {"$sum": 1},
                        "fake_count": {
                            "$sum": {"$cond": [{"$eq": ["$prediction.label", "FAKE"]}, 1, 0]}
                        },
                        "avg_confidence": {"$avg": "$prediction.confidence"}
                    }
                },
                {"$match": {"total": {"$gte": min_posts}, "fake_count": {"$gte": 1}}},
                {"$addFields": {
                    "fake_ratio": {"$divide": ["$fake_count", "$total"]}
                }},
                {"$sort": {"fake_ratio": -1}},
                {"$limit": limit}
            ]
            
            # PyMongo 4.10+ aggregate() is a coroutine, need to await it first
            cursor = await collection.aggregate(pipeline)
            results = await cursor.to_list(length=limit)
            
            return [
                {
                    "domain": item["_id"],
                    "credibility_score": None,  # Not calculated for warning sources
                    "risk_level": "VERY_HIGH" if item["fake_ratio"] > 0.5 else "HIGH",
                    "risk_color": "red" if item["fake_ratio"] > 0.5 else "orange",
                    "breakdown": {
                        "total_posts": item["total"],
                        "fake_posts": item["fake_count"],
                        "real_posts": item["total"] - item["fake_count"],
                        "fake_ratio": round(item["fake_ratio"], 4),  # Ratio 0-1
                        "fake_percentage": round(item["fake_ratio"] * 100, 2),  # Percentage 0-100
                        "avg_fake_confidence": round(item.get("avg_confidence", 0) * 100, 2),
                        "fake_avg_score": 0,  # Not calculated in this endpoint
                        "real_avg_score": 0  # Not calculated in this endpoint
                    },
                    "recommendation": "‚ö†Ô∏è Ngu·ªìn n√†y c√≥ t·ª∑ l·ªá fake news cao. C·∫ßn ki·ªÉm ch·ª©ng k·ªπ th√¥ng tin t·ª´ ngu·ªìn n√†y."
                }
                for item in results
            ]
            
        except Exception as e:
            logger.error(f"Error getting least credible sources: {e}")
            return []
    
    # ========================
    # TREND ANALYSIS
    # ========================
    
    @staticmethod
    async def get_fake_news_trend(
        days: int = 30,
        subreddit: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch xu h∆∞·ªõng fake news trong kho·∫£ng th·ªùi gian.
        
        Returns:
            - Trend direction (increasing/decreasing/stable)
            - Daily statistics
            - Peak days
            - Comparison with previous period
        """
        try:
            collection = mongodb.get_collection("reddit_posts")
            
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            prev_start_date = start_date - timedelta(days=days)
            
            # Normalize dates to start of day (00:00:00) for accurate date comparison
            # This fixes bug where date comparison fails when comparing datetime(YYYY-MM-DD 00:00:00)
            # with start_date that has time component (e.g., 23:52:03)
            start_date_normalized = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
            prev_start_date_normalized = prev_start_date.replace(hour=0, minute=0, second=0, microsecond=0)
            
            # CRITICAL FIX: created_utc trong DB l√† STRING (do post.model_dump(mode="json"))
            # nh∆∞ng query d√πng datetime object ‚Üí MongoDB kh√¥ng match string v·ªõi datetime!
            # Solution: Convert datetime query th√†nh string ISO format ƒë·ªÉ match v·ªõi string trong DB
            prev_start_date_str = prev_start_date.isoformat()
            end_date_str = end_date.isoformat()
            
            # Build match query v·ªõi string ISO format (match v·ªõi string trong DB)
            match_query = {
                "prediction": {"$exists": True, "$ne": None},
                "created_utc": {"$gte": prev_start_date_str, "$lte": end_date_str}
            }
            
            if subreddit:
                match_query["subreddit.name"] = subreddit
            
            # Aggregate by date
            # Handle c·∫£ string v√† datetime: convert string ‚Üí datetime tr∆∞·ªõc khi format
            pipeline = [
                {"$match": match_query},
                {
                    "$addFields": {
                        # Convert created_utc t·ª´ string ‚Üí datetime n·∫øu c·∫ßn
                        # V√¨ DB l∆∞u string (do model_dump(mode="json")), c·∫ßn convert ƒë·ªÉ d√πng $dateToString
                        "created_utc_datetime": {
                            "$cond": {
                                "if": {"$eq": [{"$type": "$created_utc"}, "string"]},
                                "then": {
                                    "$dateFromString": {
                                        "dateString": "$created_utc",
                                        "onError": None  # N·∫øu parse fail, return None (s·∫Ω b·ªã filter out)
                                    }
                                },
                                "else": "$created_utc"  # ƒê√£ l√† datetime r·ªìi
                            }
                        }
                    }
                },
                {
                    "$match": {
                        "created_utc_datetime": {"$ne": None}  # Filter out nh·ªØng record parse datetime fail
                    }
                },
                {
                    "$group": {
                        "_id": {
                            "date": {"$dateToString": {"format": "%Y-%m-%d", "date": "$created_utc_datetime"}},
                            "label": "$prediction.label"
                        },
                        "count": {"$sum": 1}
                    }
                },
                {"$sort": {"_id.date": 1}}
            ]
            
            # PyMongo 4.10+ aggregate() is a coroutine, need to await it first
            cursor = await collection.aggregate(pipeline)
            results = await cursor.to_list(length=10000)
            
            # Organize by date
            daily_data = {}
            for item in results:
                date = item["_id"]["date"]
                label = item["_id"]["label"]
                
                if date not in daily_data:
                    daily_data[date] = {"date": date, "fake": 0, "real": 0}
                
                if label == "FAKE":
                    daily_data[date]["fake"] = item["count"]
                else:
                    daily_data[date]["real"] = item["count"]
            
            # Split into current and previous periods
            current_period = []
            previous_period = []
            
            for date_str, data in sorted(daily_data.items()):
                date = datetime.strptime(date_str, "%Y-%m-%d")
                data["total"] = data["fake"] + data["real"]
                data["fake_percentage"] = round(
                    (data["fake"] / data["total"] * 100) if data["total"] > 0 else 0, 2
                )
                
                # Use normalized start_date for comparison to fix date boundary bug
                if date >= start_date_normalized:
                    current_period.append(data)
                else:
                    previous_period.append(data)
            
            # Calculate trends
            current_fake_total = sum(d["fake"] for d in current_period)
            current_total = sum(d["total"] for d in current_period)
            previous_fake_total = sum(d["fake"] for d in previous_period)
            previous_total = sum(d["total"] for d in previous_period)
            
            current_fake_ratio = current_fake_total / current_total if current_total > 0 else 0
            previous_fake_ratio = previous_fake_total / previous_total if previous_total > 0 else 0
            
            # Determine trend direction
            if previous_fake_ratio > 0:
                change_percentage = ((current_fake_ratio - previous_fake_ratio) / previous_fake_ratio) * 100
            else:
                change_percentage = 0
            
            if change_percentage > 5:
                trend_direction = "INCREASING"
                trend_emoji = "üìà"
            elif change_percentage < -5:
                trend_direction = "DECREASING"
                trend_emoji = "üìâ"
            else:
                trend_direction = "STABLE"
                trend_emoji = "‚û°Ô∏è"
            
            # Find peak day
            peak_day = max(current_period, key=lambda x: x["fake"]) if current_period else None
            
            # Calculate daily average
            daily_avg_fake = current_fake_total / len(current_period) if current_period else 0
            
            return {
                "period": {
                    "start": start_date.isoformat(),
                    "end": end_date.isoformat(),
                    "days": days
                },
                "trend": {
                    "direction": trend_direction,
                    "emoji": trend_emoji,
                    "change_percentage": round(change_percentage, 2),
                    "interpretation": AdvancedAnalysisService._interpret_trend(trend_direction, change_percentage)
                },
                "current_period": {
                    "total_posts": current_total,
                    "fake_posts": current_fake_total,
                    "real_posts": current_total - current_fake_total,
                    "fake_percentage": round(current_fake_ratio * 100, 2),
                    "daily_avg_fake": round(daily_avg_fake, 2)
                },
                "previous_period": {
                    "total_posts": previous_total,
                    "fake_posts": previous_fake_total,
                    "fake_percentage": round(previous_fake_ratio * 100, 2)
                },
                "peak_day": peak_day,
                "daily_data": current_period,
                "subreddit": subreddit
            }
            
        except Exception as e:
            logger.error(f"Error analyzing fake news trend: {e}")
            return {"error": str(e)}
    
    @staticmethod
    def _interpret_trend(direction: str, change: float) -> str:
        """Generate interpretation of trend."""
        if direction == "INCREASING":
            if change > 20:
                return "T·ª∑ l·ªá fake news tƒÉng ƒë√°ng b√°o ƒë·ªông! C·∫ßn tƒÉng c∆∞·ªùng ki·ªÉm ch·ª©ng th√¥ng tin."
            else:
                return "T·ª∑ l·ªá fake news c√≥ xu h∆∞·ªõng tƒÉng nh·∫π. N√™n c·∫£nh gi√°c h∆°n."
        elif direction == "DECREASING":
            if change < -20:
                return "T·ª∑ l·ªá fake news gi·∫£m ƒë√°ng k·ªÉ. M√¥i tr∆∞·ªùng th√¥ng tin ƒëang c·∫£i thi·ªán."
            else:
                return "T·ª∑ l·ªá fake news gi·∫£m nh·∫π. Xu h∆∞·ªõng t√≠ch c·ª±c."
        else:
            return "T·ª∑ l·ªá fake news ·ªïn ƒë·ªãnh. Ti·∫øp t·ª•c theo d√µi v√† ki·ªÉm ch·ª©ng th√¥ng tin."
    
    @staticmethod
    async def get_trending_fake_topics(
        days: int = 7,
        top_n: int = 20
    ) -> List[Dict[str, Any]]:
        """
        L·∫•y c√°c ch·ªß ƒë·ªÅ fake news ƒëang trending.
        Ph√¢n t√≠ch keywords trong ti√™u ƒë·ªÅ c√°c b√†i fake news g·∫ßn ƒë√¢y.
        """
        try:
            collection = mongodb.get_collection("reddit_posts")
            
            start_date = datetime.now() - timedelta(days=days)
            
            posts = await collection.find({
                "prediction.label": "FAKE",
                "prediction.confidence": {"$gte": 0.7},
                "created_utc": {"$gte": start_date}
            }).to_list(length=1000)
            
            if not posts:
                return []
            
            # Extract and count keywords
            stop_words = {
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
                'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
                'could', 'should', 'may', 'might', 'must', 'can', 'it', 'its', 'this',
                'that', 'these', 'those', 'i', 'you', 'he', 'she', 'we', 'they', 'what',
                'which', 'who', 'when', 'where', 'why', 'how', 'all', 'each', 'every',
                'says', 'said', 'new', 'just', 'after', 'before', 'video', 'news'
            }
            
            all_words = []
            for post in posts:
                title = post.get("title", "").lower()
                words = re.findall(r'\b[a-z]{4,}\b', title)
                words = [w for w in words if w not in stop_words]
                all_words.extend(words)
            
            word_counts = Counter(all_words)
            top_words = word_counts.most_common(top_n)
            
            return [
                {
                    "keyword": word,
                    "frequency": count,
                    "trending_score": round((count / len(posts)) * 100, 2),
                    "sample_titles": [
                        p["title"] for p in posts 
                        if word in p.get("title", "").lower()
                    ][:3]
                }
                for word, count in top_words
            ]
            
        except Exception as e:
            logger.error(f"Error getting trending fake topics: {e}")
            return []
    
    # ========================
    # CONTENT ANALYSIS
    # ========================
    
    @staticmethod
    async def analyze_post_content(
        post_id: str
    ) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch chi ti·∫øt n·ªôi dung c·ªßa m·ªôt post.
        
        Includes:
        - Prediction details
        - Title sentiment analysis
        - Source credibility
        - Similar fake news
        - Risk indicators
        """
        try:
            collection = mongodb.get_collection("reddit_posts")
            
            post = await collection.find_one({"post_id": post_id})
            
            if not post:
                return {"error": "Post not found", "post_id": post_id}
            
            prediction = post.get("prediction", {})
            
            # Risk indicators
            risk_indicators = []
            
            # Check title for clickbait patterns
            title = post.get("title", "").lower()
            
            clickbait_patterns = [
                "shocking", "unbelievable", "you won't believe",
                "breaking", "urgent", "exclusive", "leaked",
                "secret", "hidden truth", "they don't want you to know"
            ]
            
            for pattern in clickbait_patterns:
                if pattern in title:
                    risk_indicators.append({
                        "type": "CLICKBAIT",
                        "pattern": pattern,
                        "severity": "MEDIUM"
                    })
            
            # Check for all caps
            caps_ratio = sum(1 for c in post.get("title", "") if c.isupper()) / len(post.get("title", "a"))
            if caps_ratio > 0.3:
                risk_indicators.append({
                    "type": "EXCESSIVE_CAPS",
                    "ratio": round(caps_ratio * 100, 2),
                    "severity": "LOW"
                })
            
            # Check domain credibility
            domain = post.get("domain", "")
            domain_credibility = await AdvancedAnalysisService.get_source_credibility_score(domain)
            
            if domain_credibility.get("credibility_score") and domain_credibility["credibility_score"] < 50:
                risk_indicators.append({
                    "type": "LOW_CREDIBILITY_SOURCE",
                    "domain": domain,
                    "score": domain_credibility["credibility_score"],
                    "severity": "HIGH"
                })
            
            # Find similar posts
            similar_posts = await collection.find({
                "prediction.label": prediction.get("label"),
                "post_id": {"$ne": post_id},
                "domain": domain
            }).limit(5).to_list(length=5)
            
            # Calculate overall risk score
            risk_score = 0
            if prediction.get("label") == "FAKE":
                risk_score += prediction.get("confidence", 0) * 50
            
            for indicator in risk_indicators:
                if indicator["severity"] == "HIGH":
                    risk_score += 20
                elif indicator["severity"] == "MEDIUM":
                    risk_score += 10
                else:
                    risk_score += 5
            
            risk_score = min(100, risk_score)
            
            return {
                "post_id": post_id,
                "title": post.get("title"),
                "url": post.get("url"),
                "domain": domain,
                "created_at": post.get("created_utc"),
                "prediction": {
                    "label": prediction.get("label"),
                    "confidence": prediction.get("confidence"),
                    "confidence_percentage": round(prediction.get("confidence", 0) * 100, 2),
                    "predicted_at": prediction.get("predicted_at")
                },
                "analysis": {
                    "risk_score": round(risk_score, 2),
                    "risk_level": AdvancedAnalysisService._get_risk_level(risk_score),
                    "risk_indicators": risk_indicators,
                    "domain_credibility": domain_credibility.get("credibility_score"),
                    "domain_risk_level": domain_credibility.get("risk_level")
                },
                "engagement": {
                    "score": post.get("score", 0),
                    "num_comments": post.get("num_comments", 0),
                    "upvote_ratio": post.get("upvote_ratio", 0)
                },
                "similar_posts": [
                    {
                        "post_id": p["post_id"],
                        "title": p["title"],
                        "prediction_label": p.get("prediction", {}).get("label")
                    }
                    for p in similar_posts
                ],
                "recommendation": AdvancedAnalysisService._get_content_recommendation(
                    prediction.get("label"), 
                    risk_score
                )
            }
            
        except Exception as e:
            logger.error(f"Error analyzing post content: {e}")
            return {"error": str(e), "post_id": post_id}
    
    @staticmethod
    def _get_risk_level(score: float) -> str:
        """Convert risk score to level."""
        if score < 20:
            return "LOW"
        elif score < 40:
            return "MEDIUM"
        elif score < 60:
            return "HIGH"
        else:
            return "CRITICAL"
    
    @staticmethod
    def _get_content_recommendation(label: str, risk_score: float) -> str:
        """Generate content-specific recommendation."""
        if label == "REAL" and risk_score < 30:
            return "N·ªôi dung c√≥ v·∫ª ƒë√°ng tin c·∫≠y. V·∫´n n√™n ƒë·ªëi chi·∫øu v·ªõi c√°c ngu·ªìn kh√°c."
        elif label == "REAL" and risk_score >= 30:
            return "N·ªôi dung ƒë∆∞·ª£c ph√¢n lo·∫°i l√† th·∫≠t nh∆∞ng c√≥ m·ªôt s·ªë d·∫•u hi·ªáu ƒë√°ng nghi. N√™n ki·ªÉm ch·ª©ng k·ªπ."
        elif label == "FAKE" and risk_score < 50:
            return "N·ªôi dung c√≥ kh·∫£ nƒÉng l√† tin gi·∫£. Kh√¥ng n√™n chia s·∫ª tr∆∞·ªõc khi x√°c minh."
        else:
            return "‚ö†Ô∏è N·ªôi dung c√≥ r·ªßi ro cao l√† tin gi·∫£. Khuy·∫øn ngh·ªã KH√îNG chia s·∫ª v√† c·∫ßn ki·ªÉm ch·ª©ng t·ª´ nhi·ªÅu ngu·ªìn uy t√≠n."
    
    @staticmethod
    def _get_risk_recommendation(risk_level: str) -> str:
        """Generate risk-based recommendation."""
        recommendations = {
            "LOW": "M√¥i tr∆∞·ªùng th√¥ng tin an to√†n. Ti·∫øp t·ª•c th·ª±c h√†nh ki·ªÉm ch·ª©ng th√¥ng tin th∆∞·ªùng xuy√™n.",
            "MEDIUM": "C·∫ßn c·∫©n th·∫≠n h∆°n khi ti·∫øp nh·∫≠n th√¥ng tin. N√™n ƒë·ªëi chi·∫øu nhi·ªÅu ngu·ªìn.",
            "HIGH": "C·∫£nh b√°o! T·ª∑ l·ªá fake news cao. C·∫ßn ki·ªÉm ch·ª©ng k·ªπ m·ªçi th√¥ng tin tr∆∞·ªõc khi tin.",
            "CRITICAL": "‚ö†Ô∏è R·ª¶I RO CAO! M√¥i tr∆∞·ªùng th√¥ng tin c√≥ nhi·ªÅu tin gi·∫£. Ch·ªâ tin c√°c ngu·ªìn ƒë√£ ƒë∆∞·ª£c x√°c minh."
        }
        return recommendations.get(risk_level, "Ti·∫øp t·ª•c theo d√µi v√† ki·ªÉm ch·ª©ng th√¥ng tin.")
    
    # ========================
    # STATISTICS & REPORTS
    # ========================
    
    @staticmethod
    async def get_comprehensive_report(
        days: int = 30
    ) -> Dict[str, Any]:
        """
        T·∫°o b√°o c√°o t·ªïng h·ª£p v·ªÅ t√¨nh tr·∫°ng fake news.
        """
        try:
            collection = mongodb.get_collection("reddit_posts")
            
            start_date = datetime.now() - timedelta(days=days)
            
            # Basic stats
            pipeline = [
                {
                    "$match": {
                        "prediction": {"$exists": True, "$ne": None},
                        "created_utc": {"$gte": start_date}
                    }
                },
                {
                    "$group": {
                        "_id": "$prediction.label",
                        "count": {"$sum": 1},
                        "avg_confidence": {"$avg": "$prediction.confidence"},
                        "avg_score": {"$avg": "$score"},
                        "avg_comments": {"$avg": "$num_comments"}
                    }
                }
            ]
            
            # PyMongo 4.10+ aggregate() is a coroutine, need to await it first
            cursor = await collection.aggregate(pipeline)
            basic_stats = await cursor.to_list(length=10)
            
            # Parse stats
            stats = {"FAKE": {}, "REAL": {}}
            for item in basic_stats:
                label = item["_id"]
                if label in stats:
                    stats[label] = {
                        "count": item["count"],
                        "avg_confidence": round(item["avg_confidence"] * 100, 2),
                        "avg_score": round(item["avg_score"], 2),
                        "avg_comments": round(item["avg_comments"], 2)
                    }
            
            total = stats["FAKE"].get("count", 0) + stats["REAL"].get("count", 0)
            
            # Get top sources
            top_credible = await AdvancedAnalysisService.get_top_credible_sources(limit=5)
            least_credible = await AdvancedAnalysisService.get_least_credible_sources(limit=5)
            
            # Get trend
            trend = await AdvancedAnalysisService.get_fake_news_trend(days=days)
            
            # Get trending topics
            trending_topics = await AdvancedAnalysisService.get_trending_fake_topics(days=7, top_n=10)
            
            return {
                "report_period": {
                    "start": start_date.isoformat(),
                    "end": datetime.now().isoformat(),
                    "days": days
                },
                "summary": {
                    "total_analyzed": total,
                    "fake_news_count": stats["FAKE"].get("count", 0),
                    "real_news_count": stats["REAL"].get("count", 0),
                    "fake_percentage": round(
                        (stats["FAKE"].get("count", 0) / total * 100) if total > 0 else 0, 2
                    )
                },
                "fake_news_stats": stats["FAKE"],
                "real_news_stats": stats["REAL"],
                "trend_analysis": {
                    "direction": trend.get("trend", {}).get("direction"),
                    "change_percentage": trend.get("trend", {}).get("change_percentage"),
                    "interpretation": trend.get("trend", {}).get("interpretation")
                },
                "top_credible_sources": top_credible[:5],
                "warning_sources": least_credible[:5],
                "trending_fake_topics": trending_topics[:5],
                "recommendations": [
                    "Lu√¥n ki·ªÉm ch·ª©ng th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn uy t√≠n",
                    "C·∫©n th·∫≠n v·ªõi c√°c ngu·ªìn tin c√≥ t·ª∑ l·ªá fake news cao",
                    "Ch√∫ √Ω c√°c ch·ªß ƒë·ªÅ ƒëang c√≥ nhi·ªÅu tin gi·∫£: " + ", ".join(
                        [t["keyword"] for t in trending_topics[:3]]
                    ) if trending_topics else "N/A"
                ],
                "generated_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error generating comprehensive report: {e}")
            return {"error": str(e)}


# Global instance
advanced_analysis = AdvancedAnalysisService()

