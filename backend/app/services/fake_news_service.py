"""
Fake News Detection Service s·ª≠ d·ª•ng Hugging Face model.
Model: Pulk17/Fake-News-Detection
H·ªó tr·ª£ c·∫£ local model (transformers) v√† Inference API.
"""
import httpx
import asyncio
from typing import Dict, Optional, List, Tuple
from datetime import datetime

from app.core.config import settings
from app.core.logger import get_logger

logger = get_logger(__name__)

# Import cho local model (optional - ch·ªâ load khi c·∫ßn)
_local_model_imported = False
try:
    import torch
    import torch.nn.functional as F
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    _local_model_imported = True
except ImportError:
    logger.warning(
        "‚ö†Ô∏è  PyTorch/Transformers ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. "
        "Local model s·∫Ω kh√¥ng ho·∫°t ƒë·ªông. C√†i ƒë·∫∑t: pip install torch transformers"
    )


class FakeNewsDetectionService:
    """
    Service ƒë·ªÉ ph√°t hi·ªán fake news s·ª≠ d·ª•ng Hugging Face model.
    H·ªó tr·ª£ c·∫£ local model (transformers) v√† Inference API.
    """
    
    # Hugging Face Inference API base endpoint (model s·∫Ω ƒë∆∞·ª£c g·∫Øn ƒë·ªông theo config)
    API_BASE_URL = "https://router.huggingface.co/models"
    
    # Rate limiting settings (cho API mode)
    MAX_RETRIES = 3
    RETRY_DELAY = 2  # seconds
    TIMEOUT = 30  # seconds
    
    # Batch settings
    BATCH_SIZE = 10  # S·ªë posts x·ª≠ l√Ω c√πng l√∫c ƒë·ªÉ tr√°nh rate limit (API mode)
    BATCH_DELAY = 1  # Delay gi·ªØa c√°c batches (seconds) - ch·ªâ cho API mode
    
    # Labels mapping
    HF_LABELS = ["fake", "real"]  # Theo th·ª© t·ª± c·ªßa model
    
    def __init__(self):
        """Initialize service v·ªõi c·∫•u h√¨nh t·ª´ settings."""
        self.api_key = settings.huggingface_api_key
        self.model_name = settings.huggingface_model
        self.use_local_model = settings.use_local_hf_model
        self.device = settings.hf_model_device if _local_model_imported else "cpu"
        # Cho ph√©p override base URL qua env n·∫øu d√πng endpoint enterprise/custom
        base_url = settings.huggingface_api_base_url or self.API_BASE_URL
        # Gh√©p URL ƒë·∫ßy ƒë·ªß t·ªõi model, tr√°nh tr√πng d·∫•u '/'
        self.api_url = f"{base_url.rstrip('/')}/{self.model_name}"
        
        # Headers cho API requests (ch·ªâ d√πng khi kh√¥ng d√πng local model)
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # Local model components (kh·ªüi t·∫°o lazy)
        self.tokenizer = None
        self.local_model = None
        self._model_loaded = False
        
        # Load local model n·∫øu c·∫ßn
        if self.use_local_model:
            self._load_local_model()
        else:
            logger.info(f"üîß S·ª≠ d·ª•ng HuggingFace Inference API mode")
    
    def _load_local_model(self):
        """Load local HuggingFace model v√† tokenizer."""
        if not _local_model_imported:
            logger.error(
                "‚ùå Kh√¥ng th·ªÉ load local model: PyTorch/Transformers ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. "
                "Falling back to API mode."
            )
            self.use_local_model = False
            return
        
        if self._model_loaded:
            return
        
        try:
            logger.info(f"üì¶ ƒêang t·∫£i local model: {self.model_name}...")
            
            # Ki·ªÉm tra device
            if self.device == "cuda" and not torch.cuda.is_available():
                logger.warning("‚ö†Ô∏è  CUDA kh√¥ng kh·∫£ d·ª•ng, chuy·ªÉn sang CPU")
                self.device = "cpu"
            
            device_str = self.device if self.device == "cpu" else f"cuda:{torch.cuda.current_device()}" if torch.cuda.is_available() else "cpu"
            logger.info(f"üîß S·ª≠ d·ª•ng device: {device_str}")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Load model
            self.local_model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name
            ).to(self.device)
            self.local_model.eval()
            
            self._model_loaded = True
            logger.info(f"‚úÖ Local model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng tr√™n {device_str}")
            
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ t·∫£i local model: {e}")
            logger.warning("‚ö†Ô∏è  Falling back to API mode")
            self.use_local_model = False
            self._model_loaded = False
    
    async def _make_api_call(
        self, 
        text: str, 
        retry_count: int = 0
    ) -> Optional[Dict]:
        """
        G·ªçi Hugging Face Inference API.
        
        Args:
            text: N·ªôi dung c·∫ßn ph√¢n t√≠ch
            retry_count: S·ªë l·∫ßn retry hi·ªán t·∫°i
            
        Returns:
            Dict v·ªõi k·∫øt qu·∫£ prediction ho·∫∑c None n·∫øu th·∫•t b·∫°i
        """
        try:
            async with httpx.AsyncClient(timeout=self.TIMEOUT) as client:
                payload = {"inputs": text}
                
                response = await client.post(
                    self.api_url,
                    headers=self.headers,
                    json=payload
                )
                
                # Handle rate limiting (503)
                if response.status_code == 503:
                    error_data = response.json()
                    
                    # Model ƒëang loading
                    if "estimated_time" in error_data:
                        wait_time = error_data.get("estimated_time", 20)
                        logger.warning(
                            f"‚è≥ Model ƒëang loading, ch·ªù {wait_time}s..."
                        )
                        await asyncio.sleep(wait_time)
                        
                        # Retry
                        if retry_count < self.MAX_RETRIES:
                            return await self._make_api_call(text, retry_count + 1)
                    
                    logger.error(f"‚ùå Service unavailable: {error_data}")
                    return None
                
                # Handle other errors
                if response.status_code != 200:
                    if response.status_code in (404, 410):
                        logger.error(
                            "‚ùå HuggingFace API responded %s for model '%s'. "
                            "Double-check that the model name is correct and that the token has access. "
                            "URL: %s | response: %s",
                            response.status_code,
                            self.model_name,
                            self.api_url,
                            response.text[:200],
                        )
                    else:
                        logger.error(
                            "‚ùå HuggingFace API error %s for model '%s' via URL '%s': %s",
                            response.status_code,
                            self.model_name,
                            self.api_url,
                            response.text[:200],
                        )
                    return None
                
                # Parse response
                result = response.json()
                return result
                
        except httpx.TimeoutException:
            logger.error(f"‚è±Ô∏è  API timeout after {self.TIMEOUT}s")
            
            # Retry
            if retry_count < self.MAX_RETRIES:
                await asyncio.sleep(self.RETRY_DELAY)
                return await self._make_api_call(text, retry_count + 1)
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå API call failed: {e}")
            return None
    
    def _predict_with_local_model(self, text: str) -> Optional[Dict]:
        """
        D·ª± ƒëo√°n b·∫±ng local HuggingFace model.
        
        Args:
            text: N·ªôi dung c·∫ßn ph√¢n t√≠ch
            
        Returns:
            Dict v·ªõi prediction result
        """
        if not self._model_loaded or not self.local_model:
            logger.error("‚ùå Local model ch∆∞a ƒë∆∞·ª£c load")
            return None
        
        try:
            # Tokenize input
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True,
            ).to(self.device)
            
            # Predict
            with torch.no_grad():
                outputs = self.local_model(**inputs)
                probs = F.softmax(outputs.logits, dim=-1)[0]
            
            # T·∫°o scores dict
            scores = {
                self.HF_LABELS[i]: probs[i].item() 
                for i in range(len(self.HF_LABELS))
            }
            
            # T√¨m label c√≥ score cao nh·∫•t
            label = max(scores, key=scores.get)
            
            return {
                "label": label.upper(),  # "FAKE" ho·∫∑c "REAL"
                "confidence": round(scores[label], 4),
                "scores": {k: round(v, 4) for k, v in scores.items()},
                "predicted_at": datetime.now().isoformat(),
                "model": self.model_name,
                "method": "local"
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi predict v·ªõi local model: {e}")
            return None
    
    def _parse_prediction(self, api_result: List[List[Dict]]) -> Optional[Dict]:
        """
        Parse k·∫øt qu·∫£ t·ª´ Hugging Face API.
        
        Expected format:
        [[
            {"label": "LABEL_0", "score": 0.95},
            {"label": "LABEL_1", "score": 0.05}
        ]]
        
        LABEL_0 = REAL, LABEL_1 = FAKE
        
        Returns:
            Dict v·ªõi label, confidence, v√† scores
        """
        try:
            # API tr·∫£ v·ªÅ list of list
            if not api_result or not isinstance(api_result, list):
                logger.error("Invalid API result format")
                return None
            
            predictions = api_result[0]  # L·∫•y prediction ƒë·∫ßu ti√™n
            
            # Map label t·ª´ API format
            label_map = {
                "LABEL_0": "REAL",
                "LABEL_1": "FAKE"
            }
            
            # T·∫°o scores dict
            scores = {}
            for pred in predictions:
                label_key = label_map.get(pred["label"], pred["label"])
                scores[label_key.lower()] = pred["score"]
            
            # T√¨m prediction v·ªõi score cao nh·∫•t
            best_pred = max(predictions, key=lambda x: x["score"])
            label = label_map.get(best_pred["label"], "UNKNOWN")
            confidence = best_pred["score"]
            
            return {
                "label": label,
                "confidence": round(confidence, 4),
                "scores": {k: round(v, 4) for k, v in scores.items()},
                "predicted_at": datetime.now().isoformat(),
                "model": self.model_name,
                "method": "api"
            }
            
        except Exception as e:
            logger.error(f"Failed to parse prediction: {e}")
            return None
    
    async def predict_text(self, text: str) -> Optional[Dict]:
        """
        Predict fake news cho m·ªôt ƒëo·∫°n text.
        S·ª≠ d·ª•ng local model n·∫øu ƒë∆∞·ª£c c·∫•u h√¨nh, ng∆∞·ª£c l·∫°i d√πng API.
        
        Args:
            text: Ti√™u ƒë·ªÅ ho·∫∑c n·ªôi dung b√†i b√°o
            
        Returns:
            Dict v·ªõi prediction result ho·∫∑c None n·∫øu th·∫•t b·∫°i
            Format: {
                "label": "FAKE" | "REAL",
                "confidence": float,
                "scores": {"fake": float, "real": float},
                "predicted_at": str,
                "model": str,
                "method": "local" | "api"
            }
        """
        if not text or len(text.strip()) < 10:
            logger.warning("‚ö†Ô∏è  Text qu√° ng·∫Øn ƒë·ªÉ ph√¢n t√≠ch")
            return None
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i text
        max_length = 512
        if len(text) > max_length:
            logger.debug(f"Text d√†i {len(text)} k√Ω t·ª±, c·∫Øt xu·ªëng {max_length}")
            text = text[:max_length]
        
        # S·ª≠ d·ª•ng local model n·∫øu c√≥
        if self.use_local_model and self._model_loaded:
            logger.debug("üîß S·ª≠ d·ª•ng local model ƒë·ªÉ predict")
            return self._predict_with_local_model(text)
        
        # Fallback to API
        logger.debug("üåê S·ª≠ d·ª•ng HuggingFace API ƒë·ªÉ predict")
        api_result = await self._make_api_call(text)
        
        if not api_result:
            return None
        
        # Parse k·∫øt qu·∫£
        prediction = self._parse_prediction(api_result)
        
        return prediction
    
    async def predict_post(self, post: Dict) -> Optional[Dict]:
        """
        Predict fake news cho m·ªôt Reddit post.
        
        Args:
            post: Dict ch·ª©a th√¥ng tin post (title, selftext, ...)
            
        Returns:
            Dict v·ªõi prediction result
        """
        # T·∫°o text ƒë·ªÉ ph√¢n t√≠ch (title + selftext)
        title = post.get("title", "")
        selftext = post.get("selftext", "")
        
        # ∆Øu ti√™n title, th√™m selftext n·∫øu c√≥
        if selftext and len(selftext) > 20:
            text = f"{title}. {selftext}"
        else:
            text = title
        
        logger.info(f"üîç Predicting post: {post.get('post_id')}")
        
        prediction = await self.predict_text(text)
        
        if prediction:
            logger.info(
                f"‚úÖ Prediction: {prediction['label']} "
                f"(confidence: {prediction['confidence']:.2%})"
            )
        
        return prediction
    
    async def batch_predict_posts(
        self, 
        posts: List[Dict],
        progress_callback: Optional[callable] = None
    ) -> List[Tuple[str, Optional[Dict]]]:
        """
        Batch predict nhi·ªÅu posts v·ªõi rate limiting.
        
        Args:
            posts: List c√°c Reddit posts
            progress_callback: Function ƒë·ªÉ b√°o c√°o progress (optional)
            
        Returns:
            List of tuples (post_id, prediction_result)
        """
        results = []
        total = len(posts)
        
        logger.info(f"üöÄ Starting batch prediction for {total} posts...")
        
        # Chia th√†nh c√°c batches nh·ªè
        for i in range(0, total, self.BATCH_SIZE):
            batch = posts[i:i + self.BATCH_SIZE]
            batch_num = (i // self.BATCH_SIZE) + 1
            total_batches = (total + self.BATCH_SIZE - 1) // self.BATCH_SIZE
            
            logger.info(
                f"üì¶ Processing batch {batch_num}/{total_batches} "
                f"({len(batch)} posts)"
            )
            
            # Process batch
            batch_results = []
            for post in batch:
                post_id = post.get("post_id")
                prediction = await self.predict_post(post)
                batch_results.append((post_id, prediction))
                
                # Small delay gi·ªØa c√°c posts trong batch
                await asyncio.sleep(0.1)
            
            results.extend(batch_results)
            
            # Progress callback
            if progress_callback:
                progress = {
                    "completed": i + len(batch),
                    "total": total,
                    "percentage": ((i + len(batch)) / total) * 100
                }
                progress_callback(progress)
            
            # Delay gi·ªØa c√°c batches ƒë·ªÉ tr√°nh rate limit
            if i + self.BATCH_SIZE < total:
                logger.info(f"‚è∏Ô∏è  Waiting {self.BATCH_DELAY}s before next batch...")
                await asyncio.sleep(self.BATCH_DELAY)
        
        # Th·ªëng k√™
        successful = sum(1 for _, pred in results if pred is not None)
        failed = total - successful
        
        logger.info(
            f"‚úÖ Batch prediction completed: "
            f"{successful} successful, {failed} failed"
        )
        
        return results


# Global instance
fake_news_detector = FakeNewsDetectionService()

