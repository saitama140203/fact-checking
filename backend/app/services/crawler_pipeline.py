"""
Automated crawler pipeline v·ªõi incremental crawling.
Pipeline t·ª± ƒë·ªông: Crawl t·ª´ Reddit ‚Üí Insert tr·ª±c ti·∫øp v√†o MongoDB (kh√¥ng qua JSON).
S·ª≠ d·ª•ng PyMongo Async API (native asyncio, kh√¥ng d√πng Motor).
"""
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any
from app.core.config import settings
from app.core.logger import get_logger
from app.services.crawler import RedditCrawler
from app.services.database_service import RedditPostService, CrawlMetadataService, PredictionService
from app.services.fake_news_service import fake_news_detector
from app.services.enhanced_prediction_service import enhanced_prediction_service
from app.models.reddit import RedditPost

logger = get_logger(__name__)

class CrawlerPipeline:
    """Pipeline t·ª± ƒë·ªông crawl + save v√†o database"""
    
    def __init__(self):
        self.crawler = RedditCrawler()
        self.is_running = False
    
    async def run_incremental_crawl(self) -> Dict[str, Any]:
        """
        Ch·∫°y incremental crawl cho t·∫•t c·∫£ subreddits
        
        Returns:
            Dict v·ªõi th·ªëng k√™ crawl
        """
        if self.is_running:
            logger.warning("‚ö†Ô∏è  Crawl already running, skipping...")
            return {"status": "skipped", "reason": "already_running"}
        
        self.is_running = True
        start_time = datetime.now()
        
        try:
            logger.info("=" * 80)
            logger.info("üöÄ STARTING INCREMENTAL CRAWL PIPELINE")
            logger.info("=" * 80)
            
            all_stats = {
                "start_time": start_time,
                "subreddits": {},
                "total_crawled": 0,
                "total_inserted": 0,
                "total_duplicates": 0,
                "total_errors": 0
            }
            
            # Crawl t·ª´ng subreddit
            for subreddit_name in settings.subreddit_list:
                try:
                    logger.info(f"\nüì° Processing r/{subreddit_name}...")
                    
                    # 1. L·∫•y th·ªùi gian crawl l·∫ßn tr∆∞·ªõc
                    last_crawl_time = await CrawlMetadataService.get_last_crawl_time(subreddit_name)
                    
                    if last_crawl_time:
                        # ========================================
                        # INCREMENTAL CRAWL (ƒê√£ c√≥ data)
                        # ========================================
                        logger.info(f"   Last crawl: {last_crawl_time}")
                        time_diff = datetime.now() - last_crawl_time
                        minutes_back = int(time_diff.total_seconds() / 60) + 5
                        
                        logger.info(f"   Incremental crawl from last {minutes_back} minutes")
                        
                        # Crawl posts m·ªõi
                        posts = await self.crawler.crawl_for_analysis(
                            subreddit=subreddit_name,
                            limit=settings.posts_per_subreddit
                        )
                        
                        # Filter posts theo th·ªùi gian
                        filtered_posts = [
                            post for post in posts 
                            if post.created_utc > last_crawl_time
                        ]
                        logger.info(f"   Filtered: {len(filtered_posts)}/{len(posts)} new posts")
                        posts = filtered_posts
                        
                    else:
                        # ========================================
                        # FIRST TIME - HISTORICAL CRAWL (5 TH√ÅNG)
                        # ========================================
                        logger.info(f"   üéØ First time crawling r/{subreddit_name}")
                        logger.info(f"   üìÖ Fetching historical data from last 5 months...")
                        
                        # Crawl historical data
                        posts = await self.crawler.crawl_historical(
                            subreddit=subreddit_name,
                            months_back=settings.initial_crawl_months,
                            limit_total=settings.initial_crawl_limit
                        )
                        
                        logger.info(f"   ‚úÖ Found {len(posts)} posts from last {settings.initial_crawl_months} months")
                    
                    # 3. Save tr·ª±c ti·∫øp v√†o database (async)
                    if posts:
                        insert_stats = await RedditPostService.insert_posts_batch(posts)

                        # ƒê·∫£m b·∫£o c√≥ stats entry cho subreddit hi·ªán t·∫°i
                        if subreddit_name not in all_stats["subreddits"]:
                            all_stats["subreddits"][subreddit_name] = {}

                        # 4. Auto-predict newly inserted posts
                        if insert_stats["inserted"] > 0:
                            logger.info(
                                f"   üîç Auto-predicting {insert_stats['inserted']} new posts "
                                f"({'Enhanced HF + Gemini' if settings.enable_gemini_in_background else 'HF-only'})..."
                            )

                            predicted_count = 0
                            fake_detected = 0

                            try:
                                # L·ªçc c√°c post v·ª´a insert (ch∆∞a c√≥ prediction)
                                newly_inserted = [
                                    post for post in posts
                                    if not hasattr(post, 'prediction') or post.prediction is None
                                ][:insert_stats["inserted"]]

                                for post in newly_inserted:
                                    post_dict = post.model_dump()

                                    try:
                                        if settings.enable_gemini_in_background:
                                            # Workflow ƒë·∫ßy ƒë·ªß (HF + Gemini) ‚Äì CH·ªà khi ƒë∆∞·ª£c b·∫≠t r√µ r√†ng
                                            enhanced_result = await enhanced_prediction_service.analyze_post(post_dict)
                                            if not enhanced_result:
                                                logger.warning(
                                                    f"   ‚ö†Ô∏è  Enhanced prediction failed for post {post.post_id}, skipping..."
                                                )
                                                continue
                                            prediction = enhanced_prediction_service.format_for_database(enhanced_result)
                                        else:
                                            # M·∫∑c ƒë·ªãnh: d√πng HuggingFace-only ƒë·ªÉ tr√°nh t·ªën quota Gemini
                                            prediction = await fake_news_detector.predict_post(post_dict)
                                            if not prediction:
                                                logger.warning(
                                                    f"   ‚ö†Ô∏è  HF prediction failed for post {post.post_id}, skipping..."
                                                )
                                                continue

                                        # L∆∞u prediction v√†o DB
                                        await PredictionService.update_post_prediction(post.post_id, prediction)
                                        predicted_count += 1

                                        label = prediction.get("label") or prediction.get("label".upper(), "")
                                        if isinstance(label, str) and label.upper() == "FAKE":
                                            fake_detected += 1

                                    except Exception as e:
                                        logger.error(f"   ‚ùå Error predicting post {post.post_id}: {e}")
                                        continue

                                    # Rate limiting:
                                    # - N·∫øu c√≥ Gemini: gi·ªØ delay 5s nh∆∞ c≈© ƒë·ªÉ t√¥n tr·ªçng quota free-tier.
                                    # - N·∫øu ch·ªâ HF: delay nh·∫π ƒë·ªÉ tr√°nh spam API b√™n ngo√†i.
                                    if settings.enable_gemini_in_background:
                                        await asyncio.sleep(5.0)
                                    else:
                                        await asyncio.sleep(0.1)

                                logger.info(
                                    f"   ‚úÖ Predicted {predicted_count} posts "
                                    f"({'Enhanced' if settings.enable_gemini_in_background else 'HF-only'}), "
                                    f"detected {fake_detected} fake news"
                                )

                                all_stats["subreddits"][subreddit_name]["predicted"] = predicted_count
                                all_stats["subreddits"][subreddit_name]["fake_detected"] = fake_detected

                            except Exception as e:
                                logger.error(f"   ‚ùå Auto-prediction failed: {e}", exc_info=True)
                                all_stats["subreddits"][subreddit_name]["prediction_error"] = str(e)

                        # 5. Update last_crawl_time
                        await CrawlMetadataService.update_last_crawl_time(
                            subreddit_name,
                            datetime.now()
                        )

                        # Th·ªëng k√™
                        all_stats["subreddits"][subreddit_name].update({
                            "crawled": len(posts),
                            "inserted": insert_stats["inserted"],
                            "duplicates": insert_stats["duplicates"],
                            "errors": insert_stats["errors"]
                        })

                        all_stats["total_crawled"] += len(posts)
                        all_stats["total_inserted"] += insert_stats["inserted"]
                        all_stats["total_duplicates"] += insert_stats["duplicates"]
                        all_stats["total_errors"] += insert_stats["errors"]

                        logger.info(
                            f"   ‚úÖ Inserted: {insert_stats['inserted']}, "
                            f"Duplicates: {insert_stats['duplicates']}, "
                            f"Errors: {insert_stats['errors']}"
                        )
                    else:
                        logger.info(f"   ‚ÑπÔ∏è  No new posts found")
                        all_stats["subreddits"][subreddit_name] = {
                            "crawled": 0,
                            "inserted": 0,
                            "duplicates": 0,
                            "errors": 0
                        }
                    
                    # Rate limiting gi·ªØa c√°c subreddit
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.error(f"‚ùå Error processing r/{subreddit_name}: {e}")
                    all_stats["subreddits"][subreddit_name] = {"error": str(e)}
                    continue
            
            # K·∫øt th√∫c
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            all_stats["end_time"] = end_time
            all_stats["duration_seconds"] = duration
            all_stats["status"] = "completed"
            
            logger.info("\n" + "=" * 80)
            logger.info("‚úÖ CRAWL PIPELINE COMPLETED")
            logger.info(f"   Duration: {duration:.2f}s")
            logger.info(f"   Total crawled: {all_stats['total_crawled']}")
            logger.info(f"   Total inserted: {all_stats['total_inserted']}")
            logger.info(f"   Total duplicates: {all_stats['total_duplicates']}")
            total_in_db = await RedditPostService.get_total_posts()
            logger.info(f"   Total in DB: {total_in_db}")
            logger.info("=" * 80 + "\n")
            
            return all_stats
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "start_time": start_time
            }
        finally:
            self.is_running = False
    
    async def cleanup(self):
        """D·ªçn d·∫πp resources"""
        await self.crawler.close()