"""
User Analysis API Router - Ph√¢n t√≠ch b√†i post do ng∆∞·ªùi d√πng g·ª≠i v√†o.

H·ªó tr·ª£ 2 c√°ch:
1. G·ª≠i ƒë∆∞·ªùng link Reddit post
2. G·ª≠i tr·ª±c ti·∫øp title v√† content
"""
import re
from datetime import datetime
from typing import Optional
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field, validator

from app.services.fake_news_service import fake_news_detector
from app.services.enhanced_prediction_service import enhanced_prediction_service
from app.services.advanced_analysis_service import AdvancedAnalysisService
from app.core.logger import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/analyze", tags=["User Analysis"])


# ========================
# REQUEST MODELS
# ========================

class TextAnalysisRequest(BaseModel):
    """Request ƒë·ªÉ ph√¢n t√≠ch text tr·ª±c ti·∫øp."""
    title: str = Field(..., min_length=10, max_length=500, description="Ti√™u ƒë·ªÅ b√†i vi·∫øt")
    content: Optional[str] = Field(None, max_length=5000, description="N·ªôi dung b√†i vi·∫øt (optional)")
    source_url: Optional[str] = Field(None, description="URL ngu·ªìn (optional)")
    
    @validator('title')
    def title_not_empty(cls, v):
        if not v or not v.strip():
            raise ValueError('Title kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng')
        return v.strip()


class UrlAnalysisRequest(BaseModel):
    """Request ƒë·ªÉ ph√¢n t√≠ch t·ª´ Reddit URL."""
    url: str = Field(..., description="URL c·ªßa Reddit post")
    
    @validator('url')
    def validate_reddit_url(cls, v):
        if not v:
            raise ValueError('URL kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng')
        
        # Check if it's a valid Reddit URL
        reddit_patterns = [
            r'https?://(?:www\.)?reddit\.com/r/\w+/comments/\w+',
            r'https?://(?:old\.)?reddit\.com/r/\w+/comments/\w+',
            r'https?://(?:www\.)?redd\.it/\w+'
        ]
        
        if not any(re.match(pattern, v) for pattern in reddit_patterns):
            raise ValueError('URL kh√¥ng h·ª£p l·ªá. Vui l√≤ng s·ª≠ d·ª•ng URL Reddit.')
        
        return v


# ========================
# RESPONSE MODELS
# ========================

class AnalysisResult(BaseModel):
    """K·∫øt qu·∫£ ph√¢n t√≠ch."""
    prediction: dict = Field(..., description="K·∫øt qu·∫£ prediction")
    risk_indicators: list = Field(default=[], description="C√°c d·∫•u hi·ªáu r·ªßi ro")
    recommendation: str = Field(..., description="Khuy·∫øn ngh·ªã")
    analyzed_at: str = Field(..., description="Th·ªùi gian ph√¢n t√≠ch")


# ========================
# HELPER FUNCTIONS
# ========================

def analyze_content_risks(title: str, content: Optional[str] = None) -> list:
    """Ph√¢n t√≠ch c√°c d·∫•u hi·ªáu r·ªßi ro trong n·ªôi dung."""
    risk_indicators = []
    
    text = title.lower()
    if content:
        text += " " + content.lower()
    
    # Clickbait patterns
    clickbait_words = [
        "shocking", "unbelievable", "you won't believe", "breaking",
        "urgent", "exclusive", "leaked", "secret", "hidden truth",
        "they don't want you to know", "must see", "incredible",
        "s·ªëc", "kh√¥ng th·ªÉ tin", "kh·∫©n c·∫•p", "b√≠ m·∫≠t", "ti·∫øt l·ªô"
    ]
    
    for word in clickbait_words:
        if word in text:
            risk_indicators.append({
                "type": "CLICKBAIT_LANGUAGE",
                "description": f"Ph√°t hi·ªán t·ª´ ng·ªØ gi·∫≠t g√¢n: '{word}'",
                "severity": "MEDIUM"
            })
            break
    
    # All caps check
    caps_ratio = sum(1 for c in title if c.isupper()) / max(len(title), 1)
    if caps_ratio > 0.5:
        risk_indicators.append({
            "type": "EXCESSIVE_CAPS",
            "description": f"S·ª≠ d·ª•ng qu√° nhi·ªÅu ch·ªØ in hoa ({caps_ratio*100:.0f}%)",
            "severity": "LOW"
        })
    
    # Excessive punctuation
    exclamation_count = title.count('!') + title.count('?')
    if exclamation_count > 2:
        risk_indicators.append({
            "type": "EXCESSIVE_PUNCTUATION",
            "description": f"Qu√° nhi·ªÅu d·∫•u ch·∫•m than/h·ªèi ({exclamation_count} d·∫•u)",
            "severity": "LOW"
        })
    
    # Check for sensational phrases
    sensational = [
        "must read", "share before deleted", "msm won't tell you",
        "wake up", "open your eyes", "the truth about"
    ]
    
    for phrase in sensational:
        if phrase in text:
            risk_indicators.append({
                "type": "SENSATIONAL_LANGUAGE",
                "description": f"Ng√¥n ng·ªØ c·∫£m t√≠nh: '{phrase}'",
                "severity": "MEDIUM"
            })
            break
    
    # Very short content with no source
    if content and len(content.strip()) < 1:
        risk_indicators.append({
            "type": "INSUFFICIENT_CONTENT",
            "description": "N·ªôi dung qu√° ng·∫Øn, thi·∫øu th√¥ng tin chi ti·∫øt",
            "severity": "LOW"
        })
    
    return risk_indicators


def get_recommendation(label: str, confidence: float, risk_count: int) -> str:
    """T·∫°o khuy·∫øn ngh·ªã d·ª±a tr√™n k·∫øt qu·∫£ ph√¢n t√≠ch."""
    
    if label == "FAKE":
        if confidence > 0.85:
            return "‚ö†Ô∏è C·∫¢NH B√ÅO: B√†i vi·∫øt c√≥ kh·∫£ nƒÉng cao l√† TIN GI·∫¢. Kh√¥ng n√™n chia s·∫ª. C·∫ßn ki·ªÉm ch·ª©ng t·ª´ nhi·ªÅu ngu·ªìn uy t√≠n."
        elif confidence > 0.7:
            return "‚ö†Ô∏è CH√ö √ù: B√†i vi·∫øt c√≥ d·∫•u hi·ªáu ƒë√°ng nghi l√† tin gi·∫£. N√™n ki·ªÉm tra k·ªπ tr∆∞·ªõc khi tin."
        else:
            return "üî∂ B√†i vi·∫øt c√≥ m·ªôt s·ªë ƒë·∫∑c ƒëi·ªÉm c·ªßa tin gi·∫£. Khuy·∫øn ngh·ªã ƒë·ªëi chi·∫øu v·ªõi c√°c ngu·ªìn kh√°c."
    else:
        if risk_count > 2:
            return "‚úÖ B√†i vi·∫øt ƒë∆∞·ª£c ph√¢n lo·∫°i l√† TIN TH·∫¨T, nh∆∞ng c√≥ m·ªôt s·ªë d·∫•u hi·ªáu c·∫ßn ch√∫ √Ω. N√™n x√°c minh th√™m."
        elif risk_count > 0:
            return "‚úÖ B√†i vi·∫øt c√≥ v·∫ª ƒë√°ng tin c·∫≠y. V·∫´n n√™n ƒë·ªëi chi·∫øu v·ªõi c√°c ngu·ªìn kh√°c ƒë·ªÉ ch·∫Øc ch·∫Øn."
        else:
            return "‚úÖ B√†i vi·∫øt c√≥ v·∫ª ƒë√°ng tin c·∫≠y. Kh√¥ng ph√°t hi·ªán d·∫•u hi·ªáu ƒë√°ng nghi."


# ========================
# ENDPOINTS
# ========================

@router.post("/text", response_model=AnalysisResult)
async def analyze_text(request: TextAnalysisRequest):
    """
    **Ph√¢n t√≠ch b√†i vi·∫øt t·ª´ text do ng∆∞·ªùi d√πng nh·∫≠p.**
    
    Ng∆∞·ªùi d√πng g·ª≠i title v√† content (optional) ƒë·ªÉ ph√¢n t√≠ch.
    
    **Parameters:**
    - title: Ti√™u ƒë·ªÅ b√†i vi·∫øt (b·∫Øt bu·ªôc, 10-500 k√Ω t·ª±)
    - content: N·ªôi dung b√†i vi·∫øt (kh√¥ng b·∫Øt bu·ªôc, t·ªëi ƒëa 5000 k√Ω t·ª±)
    - source_url: URL ngu·ªìn (kh√¥ng b·∫Øt bu·ªôc)
    
    **Returns:**
    - prediction: K·∫øt qu·∫£ d·ª± ƒëo√°n (FAKE/REAL) v·ªõi confidence
    - risk_indicators: Danh s√°ch c√°c d·∫•u hi·ªáu r·ªßi ro ph√°t hi·ªán ƒë∆∞·ª£c
    - recommendation: Khuy·∫øn ngh·ªã cho ng∆∞·ªùi d√πng
    """
    try:
        logger.info(f"üìù Analyzing user-submitted text: {request.title[:50]}...")
        
        # Prepare text for analysis
        if request.content:
            full_text = f"{request.title}. {request.content}"
        else:
            full_text = request.title
        
        # Run enhanced prediction (HF + Gemini)
        enhanced_result = await enhanced_prediction_service.analyze_news(full_text)
        
        if not enhanced_result:
            raise HTTPException(
                status_code=500,
                detail="Kh√¥ng th·ªÉ ph√¢n t√≠ch b√†i vi·∫øt. Vui l√≤ng th·ª≠ l·∫°i sau."
            )
        
        # Extract prediction from enhanced result
        hf_prediction = enhanced_result.get("hf", {})
        gemini_classifier = enhanced_result.get("gemini_classifier", {})
        analysis = enhanced_result.get("analysis", "")
        
        # Use HF prediction as primary (for backward compatibility)
        prediction_label = hf_prediction.get("label", "UNKNOWN")
        prediction_confidence = hf_prediction.get("confidence", 0.0)
        
        # Analyze risks
        risk_indicators = analyze_content_risks(request.title, request.content)
        
        # Add source check if URL provided
        if request.source_url:
            # Extract domain from URL
            domain_match = re.search(r'https?://(?:www\.)?([^/]+)', request.source_url)
            if domain_match:
                domain = domain_match.group(1)
                # Check if domain is known for fake news (could be enhanced with actual database lookup)
                suspicious_domains = ['fake', 'hoax', 'satirical']
                if any(s in domain.lower() for s in suspicious_domains):
                    risk_indicators.append({
                        "type": "SUSPICIOUS_DOMAIN",
                        "description": f"Domain '{domain}' c√≥ t√™n ƒë√°ng nghi",
                        "severity": "HIGH"
                    })
        
        # Generate recommendation (use analysis from Gemini if available)
        if analysis:
            recommendation = analysis  # Use Gemini analysis as recommendation
        else:
            recommendation = get_recommendation(
                prediction_label,
                prediction_confidence,
                len(risk_indicators)
            )
        
        logger.info(f"‚úÖ Analysis complete: {prediction_label} ({prediction_confidence:.2%})")
        
        # Build response dict (AnalysisResult model doesn't support enhanced field, so return dict)
        result_dict = {
            "prediction": {
                "label": prediction_label,
                "confidence": prediction_confidence,
                "confidence_percentage": round(prediction_confidence * 100, 2),
                "model": hf_prediction.get("model", "Pulk17/Fake-News-Detection"),
                "is_fake": prediction_label == "FAKE"
            },
            "risk_indicators": risk_indicators,
            "recommendation": recommendation,
            "analyzed_at": enhanced_result.get("analyzed_at", datetime.now().isoformat()),
            # Enhanced prediction results
            "enhanced": {
                "workflow_version": enhanced_result.get("workflow_version", "2.0"),
                "hf": hf_prediction,
                "gemini_classifier": gemini_classifier,
                "analysis": analysis
            }
        }
        
        return result_dict
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing text: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/url")
async def analyze_reddit_url(request: UrlAnalysisRequest):
    """
    **Ph√¢n t√≠ch b√†i vi·∫øt t·ª´ Reddit URL.**
    
    Ng∆∞·ªùi d√πng g·ª≠i URL c·ªßa Reddit post, h·ªá th·ªëng s·∫Ω crawl v√† ph√¢n t√≠ch.
    
    **Parameters:**
    - url: URL c·ªßa Reddit post (b·∫Øt bu·ªôc)
    
    **Supported URL formats:**
    - https://www.reddit.com/r/subreddit/comments/abc123/title
    - https://old.reddit.com/r/subreddit/comments/abc123/title
    - https://redd.it/abc123
    
    **Returns:**
    - post_info: Th√¥ng tin b√†i post (title, author, subreddit, etc.)
    - prediction: K·∫øt qu·∫£ d·ª± ƒëo√°n (FAKE/REAL)
    - risk_indicators: Danh s√°ch c√°c d·∫•u hi·ªáu r·ªßi ro
    - recommendation: Khuy·∫øn ngh·ªã
    """
    try:
        logger.info(f"üîó Analyzing Reddit URL: {request.url}")
        
        # Extract post ID from URL
        post_id = None
        
        # Pattern 1: Full Reddit URL
        match = re.search(r'/comments/([a-zA-Z0-9]+)', request.url)
        if match:
            post_id = match.group(1)
        
        # Pattern 2: Short URL (redd.it)
        if not post_id:
            match = re.search(r'redd\.it/([a-zA-Z0-9]+)', request.url)
            if match:
                post_id = match.group(1)
        
        if not post_id:
            raise HTTPException(
                status_code=400,
                detail="Kh√¥ng th·ªÉ tr√≠ch xu·∫•t ID b√†i vi·∫øt t·ª´ URL. Vui l√≤ng ki·ªÉm tra l·∫°i."
            )
        
        # Import crawler
        from app.services.crawler import RedditCrawler
        
        crawler = RedditCrawler()
        
        try:
            # Get the Reddit instance
            await crawler._ensure_reddit_client()
            
            # Fetch the submission
            submission = await crawler.reddit.submission(id=post_id)
            await submission.load()
            
            # Extract post info
            post_info = {
                "post_id": submission.id,
                "title": submission.title,
                "selftext": getattr(submission, 'selftext', '')[:500] if hasattr(submission, 'selftext') else '',
                "author": str(submission.author) if submission.author else "[deleted]",
                "subreddit": str(submission.subreddit),
                "score": submission.score,
                "upvote_ratio": submission.upvote_ratio,
                "num_comments": submission.num_comments,
                "url": submission.url,
                "domain": submission.domain,
                "created_utc": datetime.fromtimestamp(submission.created_utc).isoformat(),
                "permalink": f"https://www.reddit.com{submission.permalink}"
            }
            
        except Exception as e:
            logger.error(f"Error fetching Reddit post: {e}")
            raise HTTPException(
                status_code=400,
                detail=f"Kh√¥ng th·ªÉ l·∫•y th√¥ng tin b√†i vi·∫øt. L·ªói: {str(e)}"
            )
        finally:
            await crawler.close()
        
        # Prepare text for analysis
        if post_info["selftext"]:
            full_text = f"{post_info['title']}. {post_info['selftext']}"
        else:
            full_text = post_info['title']
        
        # Run enhanced prediction (HF + Gemini)
        enhanced_result = await enhanced_prediction_service.analyze_news(full_text)
        
        if not enhanced_result:
            raise HTTPException(
                status_code=500,
                detail="Kh√¥ng th·ªÉ ph√¢n t√≠ch b√†i vi·∫øt. Vui l√≤ng th·ª≠ l·∫°i sau."
            )
        
        # Extract prediction from enhanced result
        hf_prediction = enhanced_result.get("hf", {})
        gemini_classifier = enhanced_result.get("gemini_classifier", {})
        analysis = enhanced_result.get("analysis", "")
        
        # Use HF prediction as primary (for backward compatibility)
        prediction_label = hf_prediction.get("label", "UNKNOWN")
        prediction_confidence = hf_prediction.get("confidence", 0.0)
        
        # Analyze risks
        risk_indicators = analyze_content_risks(
            post_info['title'], 
            post_info['selftext']
        )
        
        # Check domain credibility
        domain = post_info.get("domain", "")
        if domain and domain != f"self.{post_info['subreddit']}":
            try:
                domain_cred = await AdvancedAnalysisService.get_source_credibility_score(domain)
                if domain_cred.get("credibility_score") and domain_cred["credibility_score"] < 50:
                    risk_indicators.append({
                        "type": "LOW_CREDIBILITY_SOURCE",
                        "description": f"Ngu·ªìn '{domain}' c√≥ ƒëi·ªÉm tin c·∫≠y th·∫•p ({domain_cred['credibility_score']}/100)",
                        "severity": "HIGH"
                    })
            except:
                pass  # Domain check failed, ignore
        
        # Check account age (if author is not deleted)
        if post_info["author"] != "[deleted]":
            try:
                author = await crawler.reddit.redditor(post_info["author"])
                await author.load()
                account_age_days = (datetime.now() - datetime.fromtimestamp(author.created_utc)).days
                
                if account_age_days < 30:
                    risk_indicators.append({
                        "type": "NEW_ACCOUNT",
                        "description": f"T√†i kho·∫£n m·ªõi ({account_age_days} ng√†y tu·ªïi)",
                        "severity": "MEDIUM"
                    })
            except:
                pass
        
        # Generate recommendation (use analysis from Gemini if available)
        if analysis:
            recommendation = analysis  # Use Gemini analysis as recommendation
        else:
            recommendation = get_recommendation(
                prediction_label,
                prediction_confidence,
                len(risk_indicators)
            )
        
        logger.info(f"‚úÖ URL analysis complete: {prediction_label} ({prediction_confidence:.2%})")
        
        return {
            "post_info": post_info,
            "prediction": {
                "label": prediction_label,
                "confidence": prediction_confidence,
                "confidence_percentage": round(prediction_confidence * 100, 2),
                "model": hf_prediction.get("model", "Pulk17/Fake-News-Detection"),
                "is_fake": prediction_label == "FAKE"
            },
            "risk_indicators": risk_indicators,
            "recommendation": recommendation,
            "analyzed_at": enhanced_result.get("analyzed_at", datetime.now().isoformat()),
            # Enhanced prediction results
            "enhanced": {
                "workflow_version": enhanced_result.get("workflow_version", "2.0"),
                "hf": hf_prediction,
                "gemini_classifier": gemini_classifier,
                "analysis": analysis
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing URL: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/quick")
async def quick_analyze(
    title: str,
    content: Optional[str] = None
):
    """
    **Ph√¢n t√≠ch nhanh - Endpoint ƒë∆°n gi·∫£n.**
    
    Endpoint ƒë∆°n gi·∫£n cho ph√¢n t√≠ch nhanh, ch·ªâ c·∫ßn title.
    
    **Parameters:**
    - title: Ti√™u ƒë·ªÅ b√†i vi·∫øt (query param)
    - content: N·ªôi dung (query param, optional)
    
    **Returns:**
    - is_fake: Boolean
    - label: FAKE ho·∫∑c REAL
    - confidence: ƒê·ªô tin c·∫≠y (0-1)
    - warning: C·∫£nh b√°o n·∫øu l√† tin gi·∫£
    """
    try:
        if not title or len(title.strip()) < 10:
            raise HTTPException(
                status_code=400,
                detail="Title ph·∫£i c√≥ √≠t nh·∫•t 10 k√Ω t·ª±"
            )
        
        # Prepare text
        full_text = title
        if content:
            full_text = f"{title}. {content}"
        
        # Run prediction
        prediction = await fake_news_detector.predict_text(full_text)
        
        if not prediction:
            raise HTTPException(
                status_code=500,
                detail="Kh√¥ng th·ªÉ ph√¢n t√≠ch. Vui l√≤ng th·ª≠ l·∫°i."
            )
        
        is_fake = prediction["label"] == "FAKE"
        
        return {
            "is_fake": is_fake,
            "label": prediction["label"],
            "confidence": prediction["confidence"],
            "confidence_percentage": round(prediction["confidence"] * 100, 2),
            "warning": "‚ö†Ô∏è B√†i vi·∫øt n√†y c√≥ kh·∫£ nƒÉng l√† TIN GI·∫¢!" if is_fake else None,
            "analyzed_at": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in quick analyze: {e}")
        raise HTTPException(status_code=500, detail=str(e))

