"""
Crawler API Router - Endpoints ƒë·ªÉ qu·∫£n l√Ω Reddit crawler.

Endpoints:
1. Status - Xem tr·∫°ng th√°i crawler
2. Manual trigger - Ch·∫°y crawl th·ªß c√¥ng
3. Configuration - Xem/c·∫≠p nh·∫≠t c·∫•u h√¨nh
4. Statistics - Th·ªëng k√™ crawl
"""
from fastapi import APIRouter, HTTPException, Query
from typing import Optional
from datetime import datetime

from app.services.scheduler_service import crawler_scheduler
from app.services.database_service import RedditPostService, CrawlMetadataService
from app.core.config import settings
from app.core.logger import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/crawler", tags=["Crawler"])


# ========================
# STATUS & CONTROL
# ========================

@router.get("/status")
async def get_crawler_status():
    """
    **L·∫•y tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa crawler scheduler.**
    
    **Returns:**
    - is_running: Scheduler c√≥ ƒëang ch·∫°y kh√¥ng
    - next_run_time: Th·ªùi gian ch·∫°y ti·∫øp theo
    - interval_minutes: Kho·∫£ng c√°ch gi·ªØa c√°c l·∫ßn crawl
    - jobs: Danh s√°ch c√°c jobs
    """
    try:
        status = crawler_scheduler.get_status()
        
        return {
            **status,
            "subreddits": settings.subreddit_list,
            "posts_per_subreddit": settings.posts_per_subreddit
        }
        
    except Exception as e:
        logger.error(f"Error getting crawler status: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/run")
async def trigger_manual_crawl():
    """
    **Ch·∫°y crawl th·ªß c√¥ng ngay l·∫≠p t·ª©c.**
    
    Kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn schedule t·ª± ƒë·ªông.
    
    **Returns:**
    - Th·ªëng k√™ c·ªßa l·∫ßn crawl
    """
    try:
        logger.info("üî• Manual crawl triggered via API")
        stats = await crawler_scheduler.run_now()
        
        return {
            "status": "completed",
            "message": "Manual crawl completed successfully",
            "stats": stats
        }
        
    except Exception as e:
        logger.error(f"Error in manual crawl: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/start")
async def start_scheduler():
    """
    **Kh·ªüi ƒë·ªông crawler scheduler.**
    
    Scheduler s·∫Ω t·ª± ƒë·ªông crawl theo interval ƒë√£ c·∫•u h√¨nh.
    """
    try:
        if crawler_scheduler.is_started:
            return {
                "status": "already_running",
                "message": "Scheduler is already running"
            }
        
        crawler_scheduler.start()
        
        return {
            "status": "started",
            "message": "Scheduler started successfully",
            "next_run": crawler_scheduler.get_next_run_time()
        }
        
    except Exception as e:
        logger.error(f"Error starting scheduler: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/stop")
async def stop_scheduler():
    """
    **D·ª´ng crawler scheduler.**
    
    Scheduler s·∫Ω kh√¥ng t·ª± ƒë·ªông crawl n·ªØa.
    """
    try:
        if not crawler_scheduler.is_started:
            return {
                "status": "not_running",
                "message": "Scheduler is not running"
            }
        
        crawler_scheduler.stop()
        
        return {
            "status": "stopped",
            "message": "Scheduler stopped successfully"
        }
        
    except Exception as e:
        logger.error(f"Error stopping scheduler: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========================
# CONFIGURATION
# ========================

@router.get("/config")
async def get_crawler_config():
    """
    **Xem c·∫•u h√¨nh crawler hi·ªán t·∫°i.**
    
    **Returns:**
    - C√°c settings c·ªßa crawler
    """
    return {
        "subreddits": settings.subreddit_list,
        "crawl_interval_minutes": settings.crawl_interval_minutes,
        "posts_per_subreddit": settings.posts_per_subreddit,
        "initial_crawl_months": settings.initial_crawl_months,
        "initial_crawl_limit": settings.initial_crawl_limit,
        "scheduler_status": crawler_scheduler.get_status()
    }


# ========================
# STATISTICS
# ========================

@router.get("/stats")
async def get_crawler_stats():
    """
    **L·∫•y th·ªëng k√™ t·ªïng quan v·ªÅ crawler.**
    
    **Returns:**
    - T·ªïng s·ªë posts trong database
    - S·ªë posts theo t·ª´ng subreddit
    - Th·ªùi gian crawl g·∫ßn nh·∫•t
    """
    try:
        total_posts = await RedditPostService.get_total_posts()
        
        # Get posts by subreddit
        subreddit_stats = []
        for subreddit in settings.subreddit_list:
            posts = await RedditPostService.get_posts_by_subreddit(subreddit, limit=1)
            count = len(posts)  # This is just checking if we have data
            
            last_crawl = await CrawlMetadataService.get_last_crawl_time(subreddit)
            
            subreddit_stats.append({
                "subreddit": subreddit,
                "last_crawl_time": last_crawl.isoformat() if last_crawl else None,
                "has_data": count > 0
            })
        
        return {
            "total_posts": total_posts,
            "subreddits": subreddit_stats,
            "crawler_status": crawler_scheduler.get_status()
        }
        
    except Exception as e:
        logger.error(f"Error getting crawler stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stats/{subreddit}")
async def get_subreddit_crawl_stats(
    subreddit: str,
    limit: int = Query(10, ge=1, le=100)
):
    """
    **L·∫•y th·ªëng k√™ crawl cho m·ªôt subreddit c·ª• th·ªÉ.**
    
    **Returns:**
    - S·ªë posts
    - Posts g·∫ßn ƒë√¢y
    - Th·ªùi gian crawl cu·ªëi
    """
    try:
        posts = await RedditPostService.get_posts_by_subreddit(subreddit, limit=limit)
        last_crawl = await CrawlMetadataService.get_last_crawl_time(subreddit)
        
        return {
            "subreddit": subreddit,
            "last_crawl_time": last_crawl.isoformat() if last_crawl else None,
            "recent_posts_count": len(posts),
            "recent_posts": [
                {
                    "post_id": p.get("post_id"),
                    "title": p.get("title"),
                    "created_at": p.get("created_utc"),
                    "score": p.get("score"),
                    "prediction": p.get("prediction", {}).get("label")
                }
                for p in posts
            ]
        }
        
    except Exception as e:
        logger.error(f"Error getting subreddit stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========================
# POSTS
# ========================

@router.get("/posts/recent")
async def get_recent_posts(
    limit: int = Query(20, ge=1, le=100),
    subreddit: Optional[str] = Query(None, description="Filter by subreddit")
):
    """
    **L·∫•y c√°c posts g·∫ßn ƒë√¢y nh·∫•t.**
    
    **Returns:**
    - List posts sorted by created_utc
    """
    try:
        collection = mongodb.get_collection("reddit_posts")
        
        query = {}
        if subreddit:
            query["subreddit.name"] = subreddit
        
        posts = await collection.find(query).sort("created_utc", -1).limit(limit).to_list(length=limit)
        
        return {
            "count": len(posts),
            "subreddit": subreddit,
            "posts": [
                {
                    "post_id": p.get("post_id"),
                    "title": p.get("title"),
                    "domain": p.get("domain"),
                    "subreddit": p.get("subreddit", {}).get("name"),
                    "score": p.get("score"),
                    "num_comments": p.get("num_comments"),
                    "created_at": p.get("created_utc"),
                    "prediction": p.get("prediction")
                }
                for p in posts
            ]
        }
        
    except Exception as e:
        logger.error(f"Error getting recent posts: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/posts/{post_id}")
async def get_post_by_id(post_id: str):
    """
    **L·∫•y chi ti·∫øt m·ªôt post theo ID.**
    
    **Returns:**
    - Full post data
    """
    try:
        post = await RedditPostService.get_post_by_id(post_id)
        
        if not post:
            raise HTTPException(status_code=404, detail=f"Post {post_id} not found")
        
        # Remove MongoDB _id field
        if "_id" in post:
            del post["_id"]
        
        return post
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting post {post_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Import for mongodb
from app.core.database import mongodb

