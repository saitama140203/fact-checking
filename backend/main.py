"""
FastAPI Application vá»›i automated crawler pipeline.
"""
from fastapi import FastAPI
from contextlib import asynccontextmanager
from app.core.config import settings
from app.core.database import mongodb
from app.core.logger import get_logger
from app.services.scheduler_service import crawler_scheduler
from app.services.database_service import RedditPostService
import asyncio

logger = get_logger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle manager cho FastAPI app"""
    # Startup
    logger.info("ðŸš€ Starting Fake News Detector API...")
    
    # 1. Káº¿t ná»‘i database (PyMongo Async API)
    await mongodb.connect()
    
    # 2. Khá»Ÿi Ä‘á»™ng scheduler
    crawler_scheduler.start()
    
    # 3. (Optional) Cháº¡y crawl Ä‘áº§u tiÃªn ngay
    logger.info("ðŸ”„ Running initial crawl...")
    try:
        await crawler_scheduler.run_now()
    except Exception as e:
        logger.error(f"Initial crawl failed: {e}")
    
    yield
    
    # Shutdown
    logger.info("ðŸ›‘ Shutting down application...")
    crawler_scheduler.stop()
    await mongodb.close()

app = FastAPI(
    title="Fake News Detector API",
    version="1.0.0",
    description="API Ä‘á»ƒ crawl vÃ  phÃ¡t hiá»‡n fake news tá»« Reddit",
    lifespan=lifespan
)

# ========================
# API ENDPOINTS
# ========================

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Fake News Detector API",
        "status": "running",
        "version": "1.0.0"
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Test database connection
        total_posts = await RedditPostService.get_total_posts()
        
        return {
            "status": "healthy",
            "database": "connected",
            "total_posts": total_posts,
            "scheduler": crawler_scheduler.get_status()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@app.get("/crawler/status")
async def get_crawler_status():
    """Láº¥y status cá»§a crawler scheduler"""
    return crawler_scheduler.get_status()

@app.post("/crawler/run-now")
async def trigger_manual_crawl():
    """Trigger crawl thá»§ cÃ´ng ngay láº­p tá»©c"""
    stats = await crawler_scheduler.run_now()
    return stats

@app.get("/stats")
async def get_stats():
    """Thá»‘ng kÃª database vÃ  crawler"""
    total_posts = await RedditPostService.get_total_posts()
    
    return {
        "total_posts": total_posts,
        "subreddits": settings.subreddit_list,
        "crawl_interval_minutes": settings.crawl_interval_minutes,
        "crawler_status": crawler_scheduler.get_status()
    }

@app.get("/posts/subreddit/{subreddit_name}")
async def get_posts_by_subreddit(
    subreddit_name: str,
    limit: int = 20,
    skip: int = 0
):
    """Láº¥y posts theo subreddit"""
    posts = await RedditPostService.get_posts_by_subreddit(
        subreddit_name,
        limit,
        skip
    )
    return {
        "subreddit": subreddit_name,
        "count": len(posts),
        "posts": posts
    }

@app.get("/posts/{post_id}")
async def get_post_by_id(post_id: str):
    """Láº¥y post theo ID"""
    post = await RedditPostService.get_post_by_id(post_id)
    
    if not post:
        return {"error": "Post not found"}, 404
    
    return post

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.api_reload
    )

